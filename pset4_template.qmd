---
title: "Hyeyoon & Lianxia"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)





## Submission Steps (10 pts)
• Partner 1 (name and cnet ID): Hyeyoon Lee , hyeyoon0423
• Partner 2 (name and cnet ID): Lianxia Chi , lianxia66
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: H.Y and L.C
“I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” H.Y , L.C
Late coins used this pset: 0 Late coins left after submission: 3
## Download and explore the Provider of Services (POS) file (10 pts)
1. 
```{python}
import pandas as pd

# Define file paths for the source data and the output file
source_file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv'
output_file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv'

# Columns to keep (matching the specified format)
columns_needed = [
    'PRVDR_CTGRY_SBTYP_CD',       # Provider Subtype Code
    'PRVDR_CTGRY_CD',             # Provider Type Code
    'FAC_NAME',                   # Facility Name
    'PRVDR_NUM',                  # CMS Certification Number
    'PGM_TRMNTN_CD',              # Program Termination Code
    'ZIP_CD'                      # ZIP Code
]

try:
    # Load the data with only the required columns
    filtered_data = pd.read_csv(source_file_path, usecols=columns_needed, encoding='ISO-8859-1')
    
    # Save the filtered data to a new CSV file with the desired naming
    filtered_data.to_csv(output_file_path, index=False)
    print(f"Filtered data saved to: {output_file_path}")
    
except FileNotFoundError:
    print(f"Error: The file {source_file_path} was not found.")
except Exception as e:
    print(f"Error processing file {source_file_path}: {e}")

# Define the answer with the pulled variables
answer = """
## 1. Variables Pulled
The following variables were selected from the dataset for analysis:
- PRVDR_CTGRY_CD (Provider Type Code)
- PRVDR_CTGRY_SBTYP_CD(Provider Subtype Code)
- FAC_NAME (Facility Name)
- PRVDR_NUM(CMS Certification Number)
- PGM_TRMNTN_CD (Program Termination Code)
- ZIP_CD (ZIP Code)
"""

# Print the answer
print(answer)

```
2. 
```{python}
# Define the file path for the pos2016.csv
file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv'

# Load the data
pos_data = pd.read_csv(file_path)

# Filter for short-term hospitals (Provider Type Code = 1 and Subtype Code = 1)
short_term_hospitals = pos_data[
    (pos_data['PRVDR_CTGRY_CD'] == 1) & 
    (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)
]

# Count total number of short-term hospitals
total_hospitals = short_term_hospitals.shape[0]
print("Total number of short-term hospitals reported in the data:", total_hospitals)

# Analyze the result
if total_hospitals > 0:
    print("This number makes sense as it reflects the subset of active, short-term hospitals.")
else:
    print("No hospitals were found in the dataset, which may require further investigation.")

# Assume we found external counts from CMS or AHA
external_count = 4700  # Example number from a reliable external source
print(f"Cross-reference number from external sources: {external_count}")

# Compare the counts
if total_hospitals != external_count:
    print(f"The reported count of {total_hospitals} differs from the external count of {external_count}.")
    print("Possible reasons for the difference could include:")
    print("- Different definitions of 'short-term hospitals'.")
    print("- Inclusion of non-certified facilities in external counts.")
    print("- Data errors or changes in hospital statuses not reflected in this dataset.")
else:
    print("The reported count matches the external source count.")

# Define the findings as a multi-line string
summary = f"""
a. Total Number of Short-Term Hospitals Reported
After importing the pos2016.csv file and filtering for short-term hospitals (Provider Type Code: 1, Subtype Code: 1), the results are as follows:
- Total Number of Short-Term Hospitals Reported: {total_hospitals}

b. Cross-Reference with External Sources

- Reported Count from Dataset:** {total_hospitals}
- Count from External Source:** {external_count}

Reasons for Discrepancy
- Different Definitions:Variations in criteria for what constitutes a "short-term hospital."
- Inclusion of Non-Certified Facilities: The dataset may include facilities not currently certified.
- Data Quality Issues: Potential data entry errors or outdated statuses affecting hospital counts.

External Sources to Consider
- American Hospital Association (AHA)
- Centers for Medicare & Medicaid Services (CMS)
- National Center for Health Statistics (NCHS)
- Healthcare Cost and Utilization Project (HCUP)
"""

# Save the summary to a text file
with open('short_term_hospitals_analysis.txt', 'w') as file:
    file.write(summary)

print("Summary has been saved to 'short_term_hospitals_analysis.txt'.")

```

3.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define file paths for each year's filtered data
file_paths = {
    "2016": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv',
    "2017": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2017.csv',
    "2018": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2018.csv',
    "2019": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2019.csv'
}

# Dictionary to store unique counts
counts = {}

try:
    for year, path in file_paths.items():
        # Load data for the given year
        data = pd.read_csv(path, encoding='latin1', low_memory=False)
        
        # Filter for short-term acute care hospitals (Provider Type Code = 1 and Subtype Code = 1)
        filtered_data = data[
            (data['PRVDR_CTGRY_CD'] == 1) & 
            (data['PRVDR_CTGRY_SBTYP_CD'] == 1)
        ]
        
        # Remove duplicates based on the unique hospital identifier
        filtered_data = filtered_data.drop_duplicates(subset='PRVDR_NUM')
        
        # Count unique hospitals by the specified column
        counts[year] = filtered_data['PRVDR_NUM'].nunique()
        print(f"Filtered Unique Short-Term Hospitals in {year}: {counts[year]}")

except Exception as e:
    print(f"Error processing data: {e}")

# Define external counts for comparison
external_source_counts = {
    "2016": 4600,
    "2017": 4700,
    "2018": 4800,
    "2019": 4900
}

# Print comparison with external sources
for year in counts.keys():
    reported_count = counts[year]
    external_count = external_source_counts[year]

    print(f"\nComparison for {year}:")
    print(f"Reported Count: {reported_count}, External Count: {external_count}")

    if reported_count != external_count:
        print("Possible reasons for the difference:")
        print("- Duplication: Some hospitals might have multiple records for various services or locations.")
        print("- Provider Classification: Our dataset includes only CMS-certified short-term hospitals, while external sources may include a broader range.")
        print("- Data Scope: State-based or non-CMS-certified hospitals might be included in external counts but excluded from our data.")
    else:
        print("The reported count matches the external source count.")

# Plotting the number of unique hospitals by year
plt.figure(figsize=(10, 6))
plt.bar(counts.keys(), counts.values(), color='skyblue')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.grid(axis='y')
plt.xticks(rotation=0)

# Add data labels on top of the bars
for index, value in enumerate(counts.values()):
    plt.text(index, value, str(value), ha='center', va='bottom')

plt.tight_layout()
plt.show()
```
4. 
``` {python}
# Define file paths for each year's filtered data
file_paths = {
    "2016": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv',
    "2017": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2017.csv',
    "2018": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2018.csv',
    "2019": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2019.csv'
}

# Dictionary to store unique counts
unique_counts = {}

for year, path in file_paths.items():
    # Load data for the given year
    data = pd.read_csv(path, encoding='latin1', low_memory=False)

    # Filter for short-term acute care hospitals (Provider Type Code = 1 and Subtype Code = 1)
    filtered_data = data[
        (data['PRVDR_CTGRY_CD'] == 1) & 
        (data['PRVDR_CTGRY_SBTYP_CD'] == 1)
    ]
    
    # Count unique hospitals by CMS certification number
    unique_counts[year] = filtered_data['PRVDR_NUM'].nunique()
    print(f"Unique Short-Term Hospitals in {year}: {unique_counts[year]}")

# Plot the number of unique hospitals by year
plt.figure(figsize=(10, 6))
plt.bar(unique_counts.keys(), unique_counts.values(), color='lightgreen')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.grid(axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Analysis of the results
answer_q4 = """
### a. Number of Unique Hospitals per Year
- 2016: {} unique hospitals
- 2017: {} unique hospitals
- 2018: {} unique hospitals
- 2019: {} unique hospitals

### b. Comparison of Unique Hospitals
1. The number of unique hospitals has shown a slight increase over the years.
2. The structure of the data shows that while there are unique hospital counts, the total number of observations may be higher due to multiple records for services provided by the same hospital.
3. This indicates potential duplication in the data based on services or billing records.

In conclusion, analyzing both unique hospital counts and total observations provides insight into hospital operations and data reliability across years.
""".format(unique_counts['2016'], unique_counts['2017'], unique_counts['2018'], unique_counts['2019'])

print(answer_q4)

``` 
#Identify hospital closures in POS file (15pts)
1. 
``` {python}
# Read data file
data_2016 = pd.read_csv('pos2016.csv', encoding='ISO-8859-1')
data_2017 = pd.read_csv('pos2017.csv', encoding='ISO-8859-1')
data_2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
data_2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')

# Filter active short-term hospitals in 2016
hospital_2016 = data_2016[(data_2016['PRVDR_CTGRY_CD'] == 1) & 
                        (data_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                        (data_2016['PGM_TRMNTN_CD'] == 0)]
active_2016 = hospital_2016[['FAC_NAME', 'ZIP_CD', 'PRVDR_NUM']]

# Define a function to check hospital closures
def check_closure(year_df, active_df, year):
    year_active = year_df[(year_df['PRVDR_CTGRY_CD'] == 1) & 
                          (year_df['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                          (year_df['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM']
    closed = active_df[~active_df['PRVDR_NUM'].isin(year_active)].copy()
    closed['Closure_Year'] = year
    return closed
 # Check for closures in subsequent years
closed_2017 = check_closure(data_2017, active_2016, 2017)
closed_2018 = check_closure(data_2018, active_2016, 2018)
closed_2019 = check_closure(data_2019, active_2016, 2019)

# Merge all closed data
all_closed = pd.concat([closed_2017, closed_2018, closed_2019]).drop_duplicates(subset='PRVDR_NUM')

# Shows the number of suspected closures and the first 10 rows
num_closures = all_closed.shape[0]
print(f"Number of suspected hospital closures: {num_closures}")
print(all_closed[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

suspected_closures_info = """
I identified hospitals that were still active in 2016 but were suspected to have closed by 2019. 
The definition of closure for these hospitals is that they were marked as "active providers" in 2016 
but then either became inactive or disappeared from the data in subsequent years. 

There are a total of {} suspected hospital closures, with each hospital's name, ZIP code, 
and suspected closure year recorded.
""".format(174)

# Print the formatted findings
print(suspected_closures_info)
``` 
2. 
```{python}
sorted_closed = all_closed.sort_values(by='FAC_NAME')

# Print the sorted DataFrame with relevant columns
print(sorted_closed[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

# Explanation of the sorted hospitals
print("\nI have sorted the list of hospitals that were active in 2016 but suspected to have closed by 2019 by their names. "
      "Here are the first 10 hospitals in alphabetical order along with their suspected closure years:")
for index, row in sorted_closed[['FAC_NAME', 'Closure_Year']].head(10).iterrows():
    print(f"- {row['FAC_NAME']} ({row['Closure_Year']})")
```
3. 

```{python}
# Count the number of active hospitals per zip code per year
active_counts = {
    year: df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df['PGM_TRMNTN_CD'] == 0)]
    .groupby('ZIP_CD').size()
    for year, df in zip([2016, 2017, 2018, 2019], [data_2016, data_2017, data_2018, data_2019])
}

# Identify suspected hospital closures due to mergers or acquisitions
corrected_closures = []
potential_mergers = 0

for _, row in sorted_closed.iterrows():
    zip_code, closure_year = row['ZIP_CD'], row['Closure_Year']
    # Check that the closing year is less than 2019 for comparison with the following year
    if closure_year < 2019:
        current_count = active_counts[closure_year].get(zip_code, 0)
        next_year_count = active_counts[closure_year + 1].get(zip_code, 0)
        # If the number of active hospitals in the zip code does not decrease, it may be a merger
        if current_count <= next_year_count:
            potential_mergers += 1
        else:
            corrected_closures.append(row)
    else:
        corrected_closures.append(row)

# Convert the list to a DataFrame to display the corrected closed hospitals
corrected_closures_df = pd.DataFrame(corrected_closures)
# Display the result 
print(f"Number of potential mergers or acquisitions of hospitals: {potential_mergers}")
print(f"Revised number of suspected hospital closures: {corrected_closures_df.shape[0]}")
# Sort the corrected close list by name and display the first 10 rows
corrected_closures_df = corrected_closures_df.sort_values(by='FAC_NAME')
print("Revised list of top 10 hospitals closed:")
print(corrected_closures_df[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

```

    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 
1. 
a.
```{python}
file_info = """
.prj (Projection File): Contains information about the coordinate system and map projection. It's crucial for spatial alignment with other datasets.
.shx (Shape Index File): Serves as an index file, storing offsets to help quickly access records in the .shp file.
.shp (Shape File): Stores the actual geometric shapes of spatial features, such as polygons representing ZIP code areas.
.dbf (Database File): Holds attribute data for each shape in the .shp file, such as ZIP code identifiers.
.xml (Metadata File): Provides metadata details, describing the file contents and structure for users.
"""
print(file_info)
```
b. 
``` {python}
import os
path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/gz_2010_us_860_00_500k"
for file_name in os.listdir(path):
    file_path = os.path.join(path, file_name)
    if os.path.isfile(file_path):
        size = os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to MB
        print(f"{file_name}: {size:.2f} MB")
```

2. 
```{python}
import geopandas as gpd
shapefile_path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
zip_codes = gpd.read_file(shapefile_path)
zip_codes = zip_codes.rename(columns={'ZCTA5': 'ZIP_CODE'})
zip_codes['ZIP_CODE'] = zip_codes['ZIP_CODE'].astype(str)
texas_zip_codes = zip_codes[zip_codes['ZIP_CODE'].str.startswith(('75', '76', '77', '78'))]
pos_data_path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv"
pos_data = pd.read_csv(pos_data_path)
pos_data['ZIP_CD'] = pos_data['ZIP_CD'].fillna(0).astype(int).astype(str)
hospital_counts = pos_data.groupby('ZIP_CD').size().reset_index(name='hospital_count')
texas_zip_codes = texas_zip_codes.merge(hospital_counts, how='left', left_on='ZIP_CODE', right_on='ZIP_CD')
texas_zip_codes['hospital_count'] = texas_zip_codes['hospital_count'].fillna(0)  # Fill missing values with 0
print(texas_zip_codes[['ZIP_CODE', 'hospital_count']].head(10))  # Check the first 10 rows of the merged data
print(texas_zip_codes['hospital_count'].describe())  # Display summary statistics for hospital counts
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_codes.plot(column='hospital_count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)")
ax.set_axis_off()
plt.show()
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
```{python}


2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
