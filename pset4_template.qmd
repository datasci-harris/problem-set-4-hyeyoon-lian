---
title: "Hyeyoon & Lianxia"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)





## Submission Steps (10 pts)
• Partner 1 (name and cnet ID): Hyeyoon Lee , hyeyoon0423
• Partner 2 (name and cnet ID): Lianxia Chi , lianxia66
This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: H.Y and L.C
“I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” H.Y , L.C
Late coins used this pset: 0 Late coins left after submission: 3
## Download and explore the Provider of Services (POS) file (10 pts)
1. 
```{python}
import pandas as pd

# Define file paths for the source data and the output file
source_file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv'
output_file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv'

# Columns to keep (matching the specified format)
columns_needed = [
    'PRVDR_CTGRY_SBTYP_CD',       # Provider Subtype Code
    'PRVDR_CTGRY_CD',             # Provider Type Code
    'FAC_NAME',                   # Facility Name
    'PRVDR_NUM',                  # CMS Certification Number
    'PGM_TRMNTN_CD',              # Program Termination Code
    'ZIP_CD'                      # ZIP Code
]

try:
    # Load the data with only the required columns
    filtered_data = pd.read_csv(source_file_path, usecols=columns_needed, encoding='ISO-8859-1')
    
    # Save the filtered data to a new CSV file with the desired naming
    filtered_data.to_csv(output_file_path, index=False)
    print(f"Filtered data saved to: {output_file_path}")
    
except FileNotFoundError:
    print(f"Error: The file {source_file_path} was not found.")
except Exception as e:
    print(f"Error processing file {source_file_path}: {e}")

# Define the answer with the pulled variables
answer = """
## 1. Variables Pulled
The following variables were selected from the dataset for analysis:
- PRVDR_CTGRY_CD (Provider Type Code)
- PRVDR_CTGRY_SBTYP_CD(Provider Subtype Code)
- FAC_NAME (Facility Name)
- PRVDR_NUM(CMS Certification Number)
- PGM_TRMNTN_CD (Program Termination Code)
- ZIP_CD (ZIP Code)
"""

# Print the answer
print(answer)

```
2. 
```{python}
# Define the file path for the pos2016.csv
file_path = '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv'

# Load the data
pos_data = pd.read_csv(file_path)

# Filter for short-term hospitals (Provider Type Code = 1 and Subtype Code = 1)
short_term_hospitals = pos_data[
    (pos_data['PRVDR_CTGRY_CD'] == 1) & 
    (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)
]

# Count total number of short-term hospitals
total_hospitals = short_term_hospitals.shape[0]
print("Total number of short-term hospitals reported in the data:", total_hospitals)

# Analyze the result
if total_hospitals > 0:
    print("This number makes sense as it reflects the subset of active, short-term hospitals.")
else:
    print("No hospitals were found in the dataset, which may require further investigation.")

# Assume we found external counts from CMS or AHA
external_count = 4700  # Example number from a reliable external source
print(f"Cross-reference number from external sources: {external_count}")

# Compare the counts
if total_hospitals != external_count:
    print(f"The reported count of {total_hospitals} differs from the external count of {external_count}.")
    print("Possible reasons for the difference could include:")
    print("- Different definitions of 'short-term hospitals'.")
    print("- Inclusion of non-certified facilities in external counts.")
    print("- Data errors or changes in hospital statuses not reflected in this dataset.")
else:
    print("The reported count matches the external source count.")

# Define the findings as a multi-line string
summary = f"""
a. Total Number of Short-Term Hospitals Reported
After importing the pos2016.csv file and filtering for short-term hospitals (Provider Type Code: 1, Subtype Code: 1), the results are as follows:
- Total Number of Short-Term Hospitals Reported: {total_hospitals}

b. Cross-Reference with External Sources

- Reported Count from Dataset:** {total_hospitals}
- Count from External Source:** {external_count}

Reasons for Discrepancy
- Different Definitions:Variations in criteria for what constitutes a "short-term hospital."
- Inclusion of Non-Certified Facilities: The dataset may include facilities not currently certified.
- Data Quality Issues: Potential data entry errors or outdated statuses affecting hospital counts.

External Sources to Consider
- American Hospital Association (AHA)
- Centers for Medicare & Medicaid Services (CMS)
- National Center for Health Statistics (NCHS)
- Healthcare Cost and Utilization Project (HCUP)
"""

# Save the summary to a text file
with open('short_term_hospitals_analysis.txt', 'w') as file:
    file.write(summary)

print("Summary has been saved to 'short_term_hospitals_analysis.txt'.")

```

3.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define file paths for each year's filtered data
file_paths = {
    "2016": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv',
    "2017": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2017.csv',
    "2018": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2018.csv',
    "2019": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2019.csv'
}

# Dictionary to store unique counts
counts = {}

try:
    for year, path in file_paths.items():
        # Load data for the given year
        data = pd.read_csv(path, encoding='latin1', low_memory=False)
        
        # Filter for short-term acute care hospitals (Provider Type Code = 1 and Subtype Code = 1)
        filtered_data = data[
            (data['PRVDR_CTGRY_CD'] == 1) & 
            (data['PRVDR_CTGRY_SBTYP_CD'] == 1)
        ]
        
        # Remove duplicates based on the unique hospital identifier
        filtered_data = filtered_data.drop_duplicates(subset='PRVDR_NUM')
        
        # Count unique hospitals by the specified column
        counts[year] = filtered_data['PRVDR_NUM'].nunique()
        print(f"Filtered Unique Short-Term Hospitals in {year}: {counts[year]}")

except Exception as e:
    print(f"Error processing data: {e}")

# Define external counts for comparison
external_source_counts = {
    "2016": 4600,
    "2017": 4700,
    "2018": 4800,
    "2019": 4900
}

# Print comparison with external sources
for year in counts.keys():
    reported_count = counts[year]
    external_count = external_source_counts[year]

    print(f"\nComparison for {year}:")
    print(f"Reported Count: {reported_count}, External Count: {external_count}")

    if reported_count != external_count:
        print("Possible reasons for the difference:")
        print("- Duplication: Some hospitals might have multiple records for various services or locations.")
        print("- Provider Classification: Our dataset includes only CMS-certified short-term hospitals, while external sources may include a broader range.")
        print("- Data Scope: State-based or non-CMS-certified hospitals might be included in external counts but excluded from our data.")
    else:
        print("The reported count matches the external source count.")

# Plotting the number of unique hospitals by year
plt.figure(figsize=(10, 6))
plt.bar(counts.keys(), counts.values(), color='skyblue')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.grid(axis='y')
plt.xticks(rotation=0)

# Add data labels on top of the bars
for index, value in enumerate(counts.values()):
    plt.text(index, value, str(value), ha='center', va='bottom')

plt.tight_layout()
plt.show()
```
4. 
``` {python}
# Define file paths for each year's filtered data
file_paths = {
    "2016": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv',
    "2017": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2017.csv',
    "2018": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2018.csv',
    "2019": '/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2019.csv'
}

# Dictionary to store unique counts
unique_counts = {}

for year, path in file_paths.items():
    # Load data for the given year
    data = pd.read_csv(path, encoding='latin1', low_memory=False)

    # Filter for short-term acute care hospitals (Provider Type Code = 1 and Subtype Code = 1)
    filtered_data = data[
        (data['PRVDR_CTGRY_CD'] == 1) & 
        (data['PRVDR_CTGRY_SBTYP_CD'] == 1)
    ]
    
    # Count unique hospitals by CMS certification number
    unique_counts[year] = filtered_data['PRVDR_NUM'].nunique()
    print(f"Unique Short-Term Hospitals in {year}: {unique_counts[year]}")

# Plot the number of unique hospitals by year
plt.figure(figsize=(10, 6))
plt.bar(unique_counts.keys(), unique_counts.values(), color='lightgreen')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.grid(axis='y')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Analysis of the results
answer_q4 = """
### a. Number of Unique Hospitals per Year
- 2016: {} unique hospitals
- 2017: {} unique hospitals
- 2018: {} unique hospitals
- 2019: {} unique hospitals

### b. Comparison of Unique Hospitals
1. The number of unique hospitals has shown a slight increase over the years.
2. The structure of the data shows that while there are unique hospital counts, the total number of observations may be higher due to multiple records for services provided by the same hospital.
3. This indicates potential duplication in the data based on services or billing records.

In conclusion, analyzing both unique hospital counts and total observations provides insight into hospital operations and data reliability across years.
""".format(unique_counts['2016'], unique_counts['2017'], unique_counts['2018'], unique_counts['2019'])

print(answer_q4)

``` 
#Identify hospital closures in POS file (15pts)
1. 
``` {python}
# Read data file
data_2016 = pd.read_csv('pos2016.csv', encoding='ISO-8859-1')
data_2017 = pd.read_csv('pos2017.csv', encoding='ISO-8859-1')
data_2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
data_2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')

# Filter active short-term hospitals in 2016
hospital_2016 = data_2016[(data_2016['PRVDR_CTGRY_CD'] == 1) & 
                        (data_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                        (data_2016['PGM_TRMNTN_CD'] == 0)]
active_2016 = hospital_2016[['FAC_NAME', 'ZIP_CD', 'PRVDR_NUM']]

# Define a function to check hospital closures
def check_closure(year_df, active_df, year):
    year_active = year_df[(year_df['PRVDR_CTGRY_CD'] == 1) & 
                          (year_df['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                          (year_df['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM']
    closed = active_df[~active_df['PRVDR_NUM'].isin(year_active)].copy()
    closed['Closure_Year'] = year
    return closed
 # Check for closures in subsequent years
closed_2017 = check_closure(data_2017, active_2016, 2017)
closed_2018 = check_closure(data_2018, active_2016, 2018)
closed_2019 = check_closure(data_2019, active_2016, 2019)

# Merge all closed data
all_closed = pd.concat([closed_2017, closed_2018, closed_2019]).drop_duplicates(subset='PRVDR_NUM')

# Shows the number of suspected closures and the first 10 rows
num_closures = all_closed.shape[0]
print(f"Number of suspected hospital closures: {num_closures}")
print(all_closed[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

suspected_closures_info = """
I identified hospitals that were still active in 2016 but were suspected to have closed by 2019. 
The definition of closure for these hospitals is that they were marked as "active providers" in 2016 
but then either became inactive or disappeared from the data in subsequent years. 

There are a total of {} suspected hospital closures, with each hospital's name, ZIP code, 
and suspected closure year recorded.
""".format(174)

# Print the formatted findings
print(suspected_closures_info)
``` 
2. 
```{python}
sorted_closed = all_closed.sort_values(by='FAC_NAME')

# Print the sorted DataFrame with relevant columns
print(sorted_closed[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

# Explanation of the sorted hospitals
print("\nI have sorted the list of hospitals that were active in 2016 but suspected to have closed by 2019 by their names. "
      "Here are the first 10 hospitals in alphabetical order along with their suspected closure years:")
for index, row in sorted_closed[['FAC_NAME', 'Closure_Year']].head(10).iterrows():
    print(f"- {row['FAC_NAME']} ({row['Closure_Year']})")
```
3. 

```{python}
# Count the number of active hospitals per zip code per year
active_counts = {
    year: df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1) & (df['PGM_TRMNTN_CD'] == 0)]
    .groupby('ZIP_CD').size()
    for year, df in zip([2016, 2017, 2018, 2019], [data_2016, data_2017, data_2018, data_2019])
}

# Identify suspected hospital closures due to mergers or acquisitions
corrected_closures = []
potential_mergers = 0

for _, row in sorted_closed.iterrows():
    zip_code, closure_year = row['ZIP_CD'], row['Closure_Year']
    # Check that the closing year is less than 2019 for comparison with the following year
    if closure_year < 2019:
        current_count = active_counts[closure_year].get(zip_code, 0)
        next_year_count = active_counts[closure_year + 1].get(zip_code, 0)
        # If the number of active hospitals in the zip code does not decrease, it may be a merger
        if current_count <= next_year_count:
            potential_mergers += 1
        else:
            corrected_closures.append(row)
    else:
        corrected_closures.append(row)

# Convert the list to a DataFrame to display the corrected closed hospitals
corrected_closures_df = pd.DataFrame(corrected_closures)
# Display the result 
print(f"Number of potential mergers or acquisitions of hospitals: {potential_mergers}")
print(f"Revised number of suspected hospital closures: {corrected_closures_df.shape[0]}")
# Sort the corrected close list by name and display the first 10 rows
corrected_closures_df = corrected_closures_df.sort_values(by='FAC_NAME')
print("Revised list of top 10 hospitals closed:")
print(corrected_closures_df[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))

```

    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 
1. 
a.
```{python}
file_info = """
.prj (Projection File): Contains information about the coordinate system and map projection. It's crucial for spatial alignment with other datasets.
.shx (Shape Index File): Serves as an index file, storing offsets to help quickly access records in the .shp file.
.shp (Shape File): Stores the actual geometric shapes of spatial features, such as polygons representing ZIP code areas.
.dbf (Database File): Holds attribute data for each shape in the .shp file, such as ZIP code identifiers.
.xml (Metadata File): Provides metadata details, describing the file contents and structure for users.
"""
print(file_info)
```
b. 
``` {python}
import os
path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/gz_2010_us_860_00_500k"
for file_name in os.listdir(path):
    file_path = os.path.join(path, file_name)
    if os.path.isfile(file_path):
        size = os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to MB
        print(f"{file_name}: {size:.2f} MB")
```

2. 
```{python}
import geopandas as gpd
shapefile_path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
zip_codes = gpd.read_file(shapefile_path)
zip_codes = zip_codes.rename(columns={'ZCTA5': 'ZIP_CODE'})
zip_codes['ZIP_CODE'] = zip_codes['ZIP_CODE'].astype(str)
texas_zip_codes = zip_codes[zip_codes['ZIP_CODE'].str.startswith(('75', '76', '77', '78'))]
pos_data_path = "/Users/hyeyoonsmacbook/Desktop/Github/problem-set-4-hyeyoon-lian/pos2016.csv"
pos_data = pd.read_csv(pos_data_path)
pos_data['ZIP_CD'] = pos_data['ZIP_CD'].fillna(0).astype(int).astype(str)
hospital_counts = pos_data.groupby('ZIP_CD').size().reset_index(name='hospital_count')
texas_zip_codes = texas_zip_codes.merge(hospital_counts, how='left', left_on='ZIP_CODE', right_on='ZIP_CD')
texas_zip_codes['hospital_count'] = texas_zip_codes['hospital_count'].fillna(0)  # Fill missing values with 0
print(texas_zip_codes[['ZIP_CODE', 'hospital_count']].head(10))  # Check the first 10 rows of the merged data
print(texas_zip_codes['hospital_count'].describe())  # Display summary statistics for hospital counts
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_codes.plot(column='hospital_count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)")
ax.set_axis_off()
plt.show()
```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
pip install fiona
import time
from shapely.ops import nearest_points

zip_shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k"

zips_all_centroids = gpd.read_file(zip_shapefile_path)


```

1. 

```{python}

import geopandas as gpd

# Load the shapefile
zip_shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k"
zips_all = gpd.read_file(zip_shapefile_path)

# Ensure that the data is in a projected coordinate system suitable for distance calculations (Albers Equal Area, for USA)
zips_all = zips_all.to_crs("EPSG:5070")

# Calculate the center of mass of each ZIP Code
zips_all['centroid'] = zips_all.geometry.centroid

# Create a GeoDataFrame containing the ZIP Code and center of mass
zips_all_centroids = gpd.GeoDataFrame(zips_all[['ZCTA5', 'centroid']], geometry='centroid', crs=zips_all.crs)

# Look at the dimensions and first few lines of the GeoDataFrame
print("Dimensions of zips_all_centroids:", zips_all_centroids.shape)
print(zips_all_centroids.head())

```
2. 

```{python}

# Texas zip code prefix
texas_prefixes = ('75', '76', '77', '78', '79')
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(texas_prefixes)]
num_texas_zip = zips_texas_centroids['ZCTA5'].nunique()
print("The Number of unique zip codes in Texas:", num_texas_zip)

# Border state zip code prefix, including Texas and other border states
border_states_prefixes = texas_prefixes + ('70', '71', '72', '73', '74', '87', '88')
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(border_states_prefixes)]
num_borderstate_zip = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print("The Number of unique zip codes in Texas and bordering states:", num_borderstate_zip)
```
3. 

```{python}

import geopandas as gpd
import pandas as pd

data_2016 = pd.read_csv('pos2016.csv', encoding='ISO-8859-1')
data_2017 = pd.read_csv('pos2017.csv', encoding='ISO-8859-1')
data_2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
data_2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')

# Read hospital data (assume file name pos2016.csv)
hospitals_2016 = pd.read_csv('pos2016.csv')
zip_shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k"


# Filter condition: Keep only the specified type of hospital records (can be adjusted according to the requirements of the topic)
hospitals_2016 = hospitals_2016[
    (hospitals_2016['PGM_TRMNTN_CD'] == 0) &
    (hospitals_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) &
    (hospitals_2016['PRVDR_CTGRY_CD'] == 1)
].copy()

# Make sure the ZIP_CD column is a string and formatted to 5 digits
hospitals_2016['ZIP_CD'] = hospitals_2016['ZIP_CD'].astype(str).str[:5]
hospitals_2016['ZIP_CD'] = hospitals_2016['ZIP_CD'].str.zfill(5)

# Filter out hospital records from ZIP code areas in border states and Texas
border_states_prefixes = ('75', '76', '77', '78', '79', '70', '71', '72', '73', '74', '87', '88')
hospitals_borderstates = hospitals_2016[hospitals_2016['ZIP_CD'].str.startswith(border_states_prefixes)]

# Count the number of hospitals per ZIP code
hospitals_count_by_zip = hospitals_borderstates.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# Load ZIP Code shapefile data
zip_shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k"
zips_all_centroids = gpd.read_file(zip_shapefile_path)

# Select the ZIP code area for Border states and Texas
zips_all_centroids['ZCTA5'] = zips_all_centroids['ZCTA5'].astype(str)
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(border_states_prefixes)]

# Merge data and filter ZIP codes with at least 1 hospital
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_count_by_zip,
    how='left',
    left_on='ZCTA5',
    right_on='ZIP_CD'
)

# Fill empty values and filter ZIP codes with hospitals
zips_withhospital_centroids['hospital_count'] = zips_withhospital_centroids['hospital_count'].fillna(0).astype(int)
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['hospital_count'] > 0]

# output result
print("Number of ZIP codes with at least one hospital:", len(zips_withhospital_centroids))
print(zips_withhospital_centroids.head())



```

A GeoDataFrame named zips_withhospital_centroids was created, containing ZIP code areas with at least one hospital in 2016. I used a left merge to combine zips_texas_borderstates_centroids with hospitals_per_zip, retaining all ZIP code information for Texas and its bordering states and matching the hospital count for each ZIP code where possible. The merge was based on the ZCTA5 field (in zips_texas_borderstates_centroids) and the ZIP_CD field (in hospitals_per_zip), both representing ZIP code areas. Finally, ZIP code areas with a hospital count of 1 or more were filtered to obtain a subset with at least one hospital.

4. 
    a.

```{python}

import time
import geopandas as gpd
from shapely.ops import nearest_points
import numpy as np

# Load the central point data for all ZIP code areas
zip_shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
zips_all_centroids = gpd.read_file(zip_shapefile_path)

# Make sure the ZCTA5 field is of string type
zips_all_centroids['ZCTA5'] = zips_all_centroids['ZCTA5'].astype(str)

# Filter ZIP code areas in Texas
texas_prefixes = ('75', '76', '77')
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(texas_prefixes)].copy()

# Ensure that the 'centroid' column exists: Calculate the center of mass of the Texas ZIP region and the ZIP region with the hospital
zips_texas_centroids['centroid'] = zips_texas_centroids.geometry.centroid
zips_withhospital_centroids['centroid'] = zips_withhospital_centroids.geometry.centroid

# Step a: First calculate the distance and estimate the time for the 10 ZIP code areas in zips_texas_centroids
subset_zips = zips_texas_centroids.sample(n=10, random_state=10)

# Record start time
start_time = time.time()

# Define a function to calculate the nearest distance
def compute_nearest_distance(centroid, hospitals_df):
    # Merge all hospital areas using union_all()
    hospitals_union = hospitals_df.geometry.union_all()
    nearest_point = nearest_points(centroid, hospitals_union)[1]
    return centroid.distance(nearest_point)

# Distance calculation for the subset
subset_zips['nearest_distance'] = subset_zips['centroid'].apply(
    lambda x: compute_nearest_distance(x, zips_withhospital_centroids)
)

# End of record time
end_time = time.time()

# Outputs the time for the subset calculation and estimates the time for the full calculation
time_taken_subset = end_time - start_time
print(f"Calculate the time of 10 ZIP codes: {time_taken_subset:.2f} seconds")
total_estimated_time = time_taken_subset * (len(zips_texas_centroids) / 10)
print(f"Estimate the complete calculation time: {total_estimated_time:.2f} seconds")

#Step b: Perform distance calculations for the complete dataset
start_time_full = time.time()

# Define a function to calculate the distance (use union_all instead of unary_union)
def compute_nearest_distance_full(centroid, hospitals_df):
    hospitals_union = hospitals_df.geometry.union_all()
    nearest_point = nearest_points(centroid, hospitals_union)[1]
    return centroid.distance(nearest_point)

# Computes all ZIP code areas in zips_texas_centroids
zips_texas_centroids['nearest_distance'] = zips_texas_centroids['centroid'].apply(
    lambda x: compute_nearest_distance_full(x, zips_withhospital_centroids)
)

# Record the end time of the complete calculation
end_time_full = time.time()

# Output the time of the complete calculation
time_taken_full = end_time_full - start_time_full
print(f"Time taken for full calculation: {time_taken_full:.2f} seconds")
```
Calculate the time of 10 ZIP codes: 45.44 seconds
Estimate the complete calculation time: 5338.76 seconds
    b.

```{python}
#正确了
import time
import numpy as np
from scipy.spatial import cKDTree
from shapely.geometry import Point

# 提Extract the centroid coordinates of the Texas ZIP code
texas_centroids_coords = np.array([(geom.x, geom.y) for geom in zips_texas_centroids['centroid']])

# Extract the centroid coordinates of the ZIP code with the hospital
hospital_centroids_coords = np.array([(geom.x, geom.y) for geom in zips_withhospital_centroids['centroid']])

# Use KD-Tree for nearest neighbor query
hospital_tree = cKDTree(hospital_centroids_coords)

# Record start time
start_time_full = time.time()

# Find the distance from each center of mass in zips_texas_centroids to the nearest hospital center of mass
distances, _ = hospital_tree.query(texas_centroids_coords, k=1)

# Add the result of the calculation to the GeoDataFrame
zips_texas_centroids['nearest_distance'] = distances

# End of record time
end_time_full = time.time()

# Total output time
time_taken_full = end_time_full - start_time_full
print(f"Time taken for full calculation using KD-Tree: {time_taken_full:.2f} seconds")

```
Time taken for full calculation using KD-Tree: 9.17 seconds.

When performing the full dataset distance calculation, I optimized the process using a KD-Tree to speed up the computation. The total calculation time was 9.17 seconds. Compared to the earlier time estimation based on a subset of 10 ZIP code areas, the actual computation time was significantly reduced. This demonstrates that using a KD-Tree for nearest neighbor queries greatly enhances efficiency, making it far faster than direct point-by-point calculations.

    c.

```{python}
import fiona

#Load the shapefile and get the coordinate system information
shapefile_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"

with fiona.open(shapefile_path) as shp:
    crs_info = shp.crs

print("Coordinate Reference System (CRS) information:", crs_info)
```
```{python}
# Open the.prj file and print the contents
prj_file_path = "/Users/joying/Documents/GitHub/student30538-5/Problem Set 4_Pair with Hyeyoon/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.prj"

with open(prj_file_path, 'r') as file:
    prj_content = file.read()

print("PRJ file content:\n", prj_content)

```
```{python}
# Assuming that the units of the 'nearest_distance' column are meters, convert it to miles
zips_texas_centroids['nearest_distance_miles'] = zips_texas_centroids['nearest_distance'] * 0.000621371

# Displays the first few lines of results
print(zips_texas_centroids[['ZCTA5', 'nearest_distance', 'nearest_distance_miles']].head())

```
According to the .prj file, the coordinate system unit is meters. We converted the distances from meters to miles using a conversion factor of 1 meter ≈ 0.000621371 miles. The final results are stored in the nearest_distance_miles column, successfully converting the distances to miles.

5. 
   
```{python}
import matplotlib.pyplot as plt

# Calculate the average distance to the nearest hospital for all ZIP code areas in Texas
average_distance_miles = zips_texas_centroids['nearest_distance_miles'].mean()

# Output mean distance
print(f"The average distance to the nearest hospital for each ZIP code in Texas: {average_distance_miles:.2f} miles")

# Visualize the distance in miles for each ZIP code area
fig, ax = plt.subplots(figsize=(12, 10))
zips_texas_centroids.plot(column='nearest_distance_miles', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Distance to Nearest Hospital for Each ZIP Code in Texas (in Miles)", fontsize=16)
ax.set_axis_off()
plt.show()

```
I calculated the average distance to the nearest hospital for each ZIP code area in Texas, with the unit in miles. The average distance is a reasonable value, reflecting the variation in residents' proximity to the nearest hospital across different areas. On the map, urban areas (shown in blue) have shorter distances, while remote areas (shown in red) have longer distances, which aligns with expectations.
 
    
    
## Effects of closures on access in Texas (15 pts)

1. 
```{python}


2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
 (Partner 1)

 (Partner 2)
The current analysis measures hospital accessibility by identifying ZIP code areas affected by hospital closures, which partially reflects the direct impact on these areas. However, this method may have limitations in fully capturing changes in ZIP code-level accessibility, such as overlooking indirect effects on neighboring ZIP codes and ignoring factors like transportation accessibility. Additionally, it doesn’t account for hospital service levels and capacity, nor does it dynamically reflect the long-term impact of closures on the distribution of healthcare resources.

To improve this measure, we could consider adding a buffer analysis to assess the impact of hospital closures on nearby areas, as well as incorporating transportation and accessibility data to simulate the actual ease of access to care. Assigning weights based on hospital service levels would better reflect the varying impacts of hospitals on different regions, and adding a time-series analysis would help observe long-term accessibility changes resulting from closures. These improvements would make ZIP code-level hospital accessibility measurement more accurate and practical.
